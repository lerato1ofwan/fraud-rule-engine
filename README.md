# Fraud Rule Engine

A demo system for fraud detection using a custom rule engine implementation, built with .NET 8, following Domain-Driven Design (DDD), CQRS, and microservices with event-driven architecture patterns. 

Project Brief: \
Create a system that processes categorized transaction events and flags potential fraud. Apply a set of fraud rules per transaction based on different criteria and then store them in a data store. Allow the retrieval of this data via an API.

## ğŸ—ï¸ Applications

The system consists of three microservices:

1. **Transactions API**: Ingress service for injesting, receiving and storing transactions
2. **Evaluations Worker**: Background worker that evaluates transactions against fraud rules
3. **Reporting API**: Analytics service with read-optimized data models and queries

## ğŸš€ Quick Start

### Prerequisites

- .NET 8 SDK
- Docker and Docker Compose
- Git

### Initial Setup

**âš ï¸ Important**: All credentials and configuration are loaded from a `.env` file. This ensures no sensitive data is committed to version control.

1. **Clone the repository**:
   ```bash
   git clone https://github.com/your-org/fraud-rule-engine.git
   cd fraud-rule-engine
   ```

2. **Create your environment file**:

   Copy the .env.example and rename it to .env or run the below command:

   ```bash
   cp .env.example .env
   ```

3. **Configure your environment variables**:
   Open `.env` in your editor and update the following **mandatory** values:
   - `POSTGRES_PASSWORD`: Set a secure password for PostgreSQL (required)
   - `GRAFANA_ADMIN_PASSWORD`: Set a password for Grafana admin user (required)
   
   **Optional**: For development, you can use the default values provided in `.env.example`. For production, use strong, unique passwords.

   > **Note**: The `.env` file is already in `.gitignore` and will not be committed to version control.

### How To Run

1. Two options for running, a development simple version and the production stack.

   **Option 1: Simplified Setup**
   ```bash
   # 7 containers - Core functionality + basic observability
   docker-compose -f docker-compose.development.yml up -d
   ```
   This includes: PostgreSQL, Kafka, 3 application services, Prometheus, Grafana

   **Option 2: Full Production Stack**
   ```bash
   # 13 containers - Complete observability (metrics, logs, traces)
   docker-compose up -d
   ```
   This includes everything above plus: Loki, Promtail, Jaeger, Kafka UI

5. **Verify services are running**:
   ```bash
   docker compose ps
   ```

6. **View logs** (optional):
   ```bash
   docker compose logs -f
   ```

### Service Endpoints

Once all services are running, you can access:

- **Transactions API**: http://localhost:5000/swagger
- **Reporting API**: http://localhost:5001/swagger
- **Kafka UI**: http://localhost:8080
- **Prometheus**: http://localhost:9090
- **Grafana**: http://localhost:3000 (credentials from `.env`)
- **Jaegar:** http://localhost:16686/search
### Stopping Services

```bash
# Stop all services
docker compose down

# Stop and remove volumes
docker compose down -v
```

### API Documentation

- **Transactions API Swagger**: http://localhost:5000/swagger
- **Reporting API Swagger**: http://localhost:5001/swagger

## ğŸ“‹ Features

### Transactions API
- REST API for transaction ingestion
- Idempotency using external transaction IDs
- Outbox pattern for reliable transactions saving and event publishing
- Domain-Driven Design with entities and value objects
- CQRS with MediatR
- Problem Details for error handling
- Health checks
- Produces transactions.receieved messages to kafka to rule set evaluations

### Evaluations Worker
- Kafka consumer for transaction events
- Fraud rule engine with Strategy pattern
- Composite rule pipeline
- Specification pattern for conditional logic
- Request/RequestHandler Mediator pattern for data loading required by rules
- Multiple fraud rules to flag transactions
- Kafka producer for fraud assessment events
- Exponential backoffs on producer and consumer failures to store in dead letter queue

### Reporting API
- Kafka consumer for fraud assessment events
- Event projections for read models
- CQRS read side with optimized queries
- Analytics endpoints (summary, daily stats, top rules)
- Implementations and makes data evaluations to grafana dashboard (via prometheus metrics)

### Infrastructure
- **Single PostgreSQL instance** shared by all services, with separate databases per application:
  - `transactions_db` - Transactions API database
  - `fraud_rule_engine_db` - Evaluations Worker database
  - `reporting_db` - Reporting API database
- Apache Kafka with Zookeeper
- Kafka UI for topic management
- Prometheus for metrics
- Grafana for visualization
- Docker Compose orchestration

## ğŸ§ª Testing (Unit Tests and Integration Testing)

Given the initial 8-10 days development constraint, my unit testing and integrations testing strategy focuses on only the most critical layers:

### Domain Logic Integrity (Unit Tests)
Focused on `FraudRuleEngine.Core` to ensure the set of fraud rules (e.g., `HighAmountRule`, `VelocityRule`, `ForeignCountryRule`) are mathematically and logically correct. Tests cover boundary conditions, edge cases (missing metadata, time window variations), and risk score aggregation logic. 

### End-to-End (Integration Tests)
Implemented in `FraudRuleEngine.Transactions.Api.Tests` using `WebApplicationFactory` and **Testcontainers** with real PostgreSQL databases using TestContainers. This validates the full DI, middleware pipeline, database interactions, and idempotency check.

**Why:**
- Prioritized domain rules and pipeline orchestration over controller unit tests (which test framework behavior rather than business logic)
- Chose integration tests over shallow unit tests to validate actual database persistence and event flow
- Used real PostgreSQL containers to catch EF Core mapping issues and migration problems that in-memory databases miss
- Future work should include increasing test code coverage to include Reporting.Api and Evaluations.Worker codebases

**Running Tests:**
```bash
# Run all tests
dotnet test

# Run only unit tests
dotnet test --filter "FullyQualifiedName~Core.Tests"

# Run only integration tests (requires Docker)
dotnet test --filter "FullyQualifiedName~Transactions.Api.Tests"
```

<!-- **Test Coverage:**
-  -->

### API Testing Examples

#### Create a Transaction

```bash
curl -X POST http://localhost:5000/api/transactions \
  -H "Content-Type: application/json" \
  -d '{
    "accountId": "123e4567-e89b-12d3-a456-426614174000",
    "amount": 15000,
    "merchantId": "123e4567-e89b-12d3-a456-426614174001",
    "currency": "ZAR",
    "timestamp": "2024-01-01T12:00:00Z",
    "externalId": "txn-12345",
    "metadata": {
      "Country": "RSA",
      "IPAddress": "192.168.1.1"
    }
  }'
```

#### Get Fraud Summary

```bash
curl http://localhost:5001/api/fraud-reports/summary/{transactionId}
```

#### Get Daily Stats

```bash
curl http://localhost:5001/api/fraud-reports/stats/daily?date=2024-01-01
```

#### Get Top Rules

```bash
curl http://localhost:5001/api/fraud-reports/rules/top?top=10
```

### Generating Test Data with Postman Collection

To populate the system with test data for visualization in Grafana and Kafka UI, you can use the included Postman collection.

**Prerequisites**:
- Postman installed (or use the Postman CLI/Newman or Insomnia depending on preferrence)
- Services running (see [How To Run](#how-to-run) above)

**Using the Collection**:

1. **Import the collection**:
   - Open Postman
   - Click "Import" and select `transactions-import-collection.js` from the project root
   - The collection will appear as "Rule Engine Transaction Load Test"

2. **Run the collection**:
   - Open the collection
   - Click "Run" (or use the Runner)
   - Set iterations to **100** (or more for more data)
   - Click "Run Rule Engine Transaction Load Test"

3. **What it does**:
   - Generates random transaction data for each iteration
   - Random amounts (1-10,000), currencies, countries, IP addresses
   - 70% bias towards ZAR/RSA (South African context)
   - Unique external IDs in various formats
   - Sends POST requests to `http://localhost:5000/api/transactions`

4. **After running**:
   - Check Grafana dashboards for metrics and visualizations
   - View Kafka topics in Kafka UI (`http://localhost:8080`)
   - Check fraud reports via the Reporting API
   - Query fraud statistics and top rules

**Note**: The first run will show empty dashboards until transactions are processed. After running 100+ iterations, you should see:
- Transaction metrics in Grafana
- Events flowing through Kafka topics
- Fraud assessments being generated
- Reporting data populated

**Alternative: Using Newman (Postman CLI)**:
```bash
# Install Newman globally
npm install -g newman

# Run the collection
newman run transactions-import-collection.js -n 100
```

## ğŸ“š Documentation

- [Architecture Documentation](docs/architecture.md)
- [Event Documentation](docs/events.md)
- [Rules Documentation](docs/rules.md)

## ğŸ› ï¸ Tech Stack

- **.NET 8**: Runtime and SDK
- **EF Core**: Object-relational mapping
- **MediatR**: CQRS implementation
- **Confluent.Kafka**: Kafka client
- **Polly**: Resilience and fault tolerance
- **PostgreSQL**: Database
- **Docker**: Containerization
- **Prometheus & Grafana**: Monitoring

<!-- ## ğŸ›ï¸ Design Patterns

- **Domain-Driven Design (DDD)**: Entities, value objects, domain events
- **CQRS**: Command/Query separation
- **Event-Driven Architecture**: Kafka-based messaging
- **Strategy Pattern**: Fraud rules
- **Composite Pattern**: Rule pipeline
- **Specification Pattern**: Conditional logic
- **Repository Pattern**: Data access abstraction
- **Outbox Pattern**: Reliable event publishing -->

## ğŸ“ Project Structure

The project follows a clean architecture with clear separation of concerns:

```
fraud-rule-engine/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ FraudRuleEngine.Transactions.Api/          
â”‚   â”‚   â”œâ”€â”€ Controllers/                            
â”‚   â”‚   â”œâ”€â”€ Data/                                  
â”‚   â”‚   â”‚   â”œâ”€â”€ Migrations/                        
â”‚   â”‚   â”‚   â”œâ”€â”€ Models/                             
â”‚   â”‚   â”‚   â”œâ”€â”€ Repositories/                       
â”‚   â”‚   â”‚   â””â”€â”€ UnitOfWork/                         
â”‚   â”‚   â”œâ”€â”€ Domain/                                  
â”‚   â”‚   â”‚   â”œâ”€â”€ DTOs/                               
â”‚   â”‚   â”‚   â”œâ”€â”€ Events/                             
â”‚   â”‚   â”‚   â””â”€â”€ ValueObjects/                       
â”‚   â”‚   â”œâ”€â”€ Services/                              
â”‚   â”‚   â”‚   â”œâ”€â”€ Behaviours/                         
â”‚   â”‚   â”‚   â”œâ”€â”€ Commands/                           
â”‚   â”‚   â”‚   â”œâ”€â”€ Queries/                            
â”‚   â”‚   â”‚   â””â”€â”€ Messaging/                          
â”‚   â”‚   â”œâ”€â”€ Dockerfile                              
â”‚   â”‚   â””â”€â”€ Program.cs                              
â”‚   â”‚
â”‚   â”œâ”€â”€ FraudRuleEngine.Evaluations.Worker/         
â”‚   â”‚   â”œâ”€â”€ Data/                                   
â”‚   â”‚   â”‚   â”œâ”€â”€ Migrations/                         
â”‚   â”‚   â”‚   â”œâ”€â”€ Models/                             
â”‚   â”‚   â”‚   â”œâ”€â”€ Repositories/                       
â”‚   â”‚   â”‚   â””â”€â”€ Requests/                           
â”‚   â”‚   â”œâ”€â”€ Services/                               
â”‚   â”‚   â”œâ”€â”€ Workers/                                
â”‚   â”‚   â”œâ”€â”€ Dockerfile                              
â”‚   â”‚   â””â”€â”€ Program.cs                              
â”‚   â”‚
â”‚   â”œâ”€â”€ FraudRuleEngine.Reporting.Api/              
â”‚   â”‚   â”œâ”€â”€ Controllers/                           
â”‚   â”‚   â”œâ”€â”€ Data/                                   
â”‚   â”‚   â”‚   â”œâ”€â”€ Migrations/                         
â”‚   â”‚   â”‚   â”œâ”€â”€ Models/                             
â”‚   â”‚   â”‚   â””â”€â”€ Repositories/                       
â”‚   â”‚   â”œâ”€â”€ Domain/                                 
â”‚   â”‚   â”‚   â”œâ”€â”€ DTOs/                               
â”‚   â”‚   â”‚   â””â”€â”€ ReadModels/                         
â”‚   â”‚   â”œâ”€â”€ Services/                               
â”‚   â”‚   â”‚   â”œâ”€â”€ Metrics/                            
â”‚   â”‚   â”‚   â”œâ”€â”€ Projections/                        
â”‚   â”‚   â”‚   â””â”€â”€ Queries/                            
â”‚   â”‚   â”œâ”€â”€ Workers/                                
â”‚   â”‚   â”œâ”€â”€ Metrics/                                
â”‚   â”‚   â”œâ”€â”€ Dockerfile                              
â”‚   â”‚   â””â”€â”€ Program.cs                              
â”‚   â”‚
â”‚   â”œâ”€â”€ FraudRuleEngine.Core/                       
â”‚   â”‚   â””â”€â”€ Domain/                                 
â”‚   â”‚       â”œâ”€â”€ Rules/                              
â”‚   â”‚       â”œâ”€â”€ Specifications/                    
â”‚   â”‚       â”œâ”€â”€ ValueObjects/                      
â”‚   â”‚       â”œâ”€â”€ DataRequests/                      
â”‚   â”‚       â”œâ”€â”€ CompositeRulePipeline.cs           
â”‚   â”‚       â””â”€â”€ IFraudRule.cs                      
â”‚   â”‚
â”‚   â””â”€â”€ FraudRuleEngine.Shared/                    
â”‚       â”œâ”€â”€ Common/                                
â”‚       â”‚   â””â”€â”€ Result.cs                          
â”‚       â”œâ”€â”€ Contracts/                             
â”‚       â”œâ”€â”€ Events/                                
â”‚       â”œâ”€â”€ Messaging/                             
â”‚       â””â”€â”€ Metrics/                               
â”‚
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ FraudRuleEngine.Core.Tests/                 
â”‚   â”‚   â”œâ”€â”€ Domain/                                
â”‚   â”‚   â””â”€â”€ Helpers/                            
â”‚   â”‚
â”‚   â””â”€â”€ FraudRuleEngine.Transactions.Api.Tests/    
â”‚       â”œâ”€â”€ Abstractions/                           
â”‚       â””â”€â”€ Integration/                           
â”‚
â”œâ”€â”€ infrastructure/                                  
â”‚   â”œâ”€â”€ grafana/                                    
â”‚   â”‚   â”œâ”€â”€ dashboards/                             
â”‚   â”‚   â””â”€â”€ provisioning/                          
â”‚   â”œâ”€â”€ kafka/                                    
â”‚   â”œâ”€â”€ prometheus/                                
â”‚   â””â”€â”€ promtail/                                   
â”‚
â”œâ”€â”€ docs/                                           # Documentation
â”‚   â”œâ”€â”€ architecture.md                            
â”‚   â”œâ”€â”€ events.md                                   # Event schema documentation
â”‚   â”œâ”€â”€ rules.md                                    # Fraud rules documentation
â”‚   â””â”€â”€ Rules and the Evaluation Service.md         # Rule engine design patterns
â”‚
â”œâ”€â”€ docker-compose.yaml                              
â”œâ”€â”€ docker-compose.development.yml                  
â”œâ”€â”€ FraudRuleEngine.sln                             
â”œâ”€â”€ .env.example                                   
â”œâ”€â”€ README-DEV.md                                   
â”œâ”€â”€ README.md                                   
â””â”€â”€ transactions-import-collection.js              # Postman/API collection for testing
```

## ğŸ”§ Development

### Building the Solution

```bash
dotnet build FraudRuleEngine.sln
```

### Running Migrations

Migrations are automatically applied on startup. To create a new migration:

```bash
# Transactions API
cd src/FraudRuleEngine.Transactions.Api
dotnet ef migrations add MigrationName --context TransactionDbContext -o Data/Migrations

# Evaluations Worker
cd src/FraudRuleEngine.Evaluations.Worker
dotnet ef migrations add MigrationName --context FraudDbContext -o Data/Migrations

# Reporting API
cd src/FraudRuleEngine.Reporting.Api
dotnet ef migrations add MigrationName --context FraudReportingDbContext -o Data/Migrations
```

## ğŸ“Š Monitoring

- **Health Checks**: `/health` endpoint on each API
- **Prometheus Metrics**: Available at http://localhost:9090
- **Grafana Dashboards**: Pre-configured dashboards at http://localhost:3000

<!-- ## ğŸš§ Future Enhancements

- Increase test coverage 
- Event replay capability
- More sophisticated fraud rules
- Machine learning integration - might consider adding a fastapi + scikit learn model
- Real-time alerting
- API authentication/authorization 
- Schema registry for Kafka events -->